{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f0a8f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T13:22:42.227452Z",
     "start_time": "2021-05-25T13:22:42.222872Z"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd859c8a",
   "metadata": {},
   "source": [
    "Collab specific stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fdc7b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:36:39.098534Z",
     "start_time": "2021-05-26T08:36:39.094154Z"
    }
   },
   "outputs": [],
   "source": [
    "running_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dab1b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:36:39.913725Z",
     "start_time": "2021-05-26T08:36:39.907002Z"
    }
   },
   "outputs": [],
   "source": [
    "if running_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    logs_directory = 'drive/MyDrive/summaries'\n",
    "    !pip install -r \"drive/MyDrive/requirements.txt\"\n",
    "else:\n",
    "    logs_directory = 'summaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3715865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:44:26.531364Z",
     "start_time": "2021-05-26T07:44:25.843849Z"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b001406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:44:17.731972Z",
     "start_time": "2021-05-26T07:44:12.178625Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gym_2048\n",
    "from tensorforce import Agent, Environment\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4308b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T13:23:07.931146Z",
     "start_time": "2021-05-25T13:23:07.926389Z"
    }
   },
   "source": [
    "# Useful code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5deab7",
   "metadata": {},
   "source": [
    "Render \"beautiful\" 2048 grid:\n",
    "\n",
    "`imshow(environment._environment.environment.render(mode=\"rgb_array\"))`\n",
    "\n",
    "Illegal move reward:\n",
    "\n",
    "`environment._environment.environment.set_illegal_move_reward(-10)`\n",
    "\n",
    "Log2 reward:\n",
    "\n",
    "`log2_reward = reward if reward <= 0 else np.log2(reward)`\n",
    "\n",
    "Show TensorBoard graphs:\n",
    "\n",
    "`%tensorboard --logdir summaries` (with parameter `summarizer=dict(directory='summaries')` in the `Agent.create()` method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70828d2c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Hyperparams to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dfb673",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`exploration`\n",
    "\n",
    "`learning_rate`\n",
    "\n",
    "Negative reward for illegal moves: `environment._environment.environment.set_illegal_move_reward(-1)`\n",
    "\n",
    "`target_sync_frequency`\n",
    "\n",
    "`batch_size` & `update_frequency`\n",
    "\n",
    "Constant reward / Log reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57f9a1",
   "metadata": {},
   "source": [
    "# Custom hyper params train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d3174",
   "metadata": {},
   "source": [
    "Modified hyperparams :\n",
    "\n",
    "Ex: `learning_rate = 0.01` et `batch_size = 32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b0ac30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:35:19.733403Z",
     "start_time": "2021-05-26T09:34:44.599993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: terminal = 1, updates=89, max_tile=128, valid_moves=129, invalid_moves=232, seconds=10.44\n",
      "Episode 1: terminal = 1, updates=54, max_tile=64, valid_moves=87, invalid_moves=131, seconds=1.33\n",
      "Episode 2: terminal = 1, updates=43, max_tile=64, valid_moves=97, invalid_moves=76, seconds=1.49\n",
      "Episode 3: terminal = 1, updates=127, max_tile=128, valid_moves=171, invalid_moves=338, seconds=3.83\n",
      "Episode 4: terminal = 1, updates=115, max_tile=128, valid_moves=145, invalid_moves=316, seconds=3.66\n",
      "Episode 5: terminal = 1, updates=46, max_tile=64, valid_moves=64, invalid_moves=120, seconds=1.4\n",
      "Episode 6: terminal = 1, updates=179, max_tile=256, valid_moves=189, invalid_moves=528, seconds=5.34\n",
      "Episode 7: terminal = 1, updates=64, max_tile=64, valid_moves=95, invalid_moves=162, seconds=1.49\n",
      "Episode 8: terminal = 1, updates=87, max_tile=64, valid_moves=103, invalid_moves=246, seconds=2.95\n",
      "Episode 9: terminal = 1, updates=74, max_tile=64, valid_moves=103, invalid_moves=193, seconds=2.8\n",
      "Last 100 episodes mean score:  1079.2\n",
      "Max tile on last 100 episodes:  256\n",
      "Total training time (minutes):  0.58\n"
     ]
    }
   ],
   "source": [
    "environment = Environment.create(\n",
    "    environment='gym', level='2048-v0', max_episode_timesteps=1000\n",
    ")\n",
    "\n",
    "# Defaut hyperparams\n",
    "agent = Agent.create(\n",
    "    agent='dqn',\n",
    "    batch_size=16, # Required by Tensorforce\n",
    "    update_frequency=4, # Update frequency, TensorForce default : batch_size * 0.25\n",
    "    environment=environment,\n",
    "    learning_rate = 0.001, # (TensorForce default)\n",
    "    discount = 0.99, # (TensorForce default)\n",
    "    memory=10000,\n",
    "    exploration=0.1, # (0 is the TensorForce default)\n",
    "    target_sync_frequency=4, # (1 is the TensorForce default)\n",
    "    summarizer=dict(\n",
    "        directory=logs_directory,\n",
    "        summaries=[\n",
    "            \"regularization-loss\", \n",
    "            \"loss\",\n",
    "            \"episode-length\",\n",
    "            \"episode-reward\",\n",
    "            \"objective-loss\",\n",
    "            \"reward\",\n",
    "            \"update-return\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "max_tiles = []\n",
    "scores = []\n",
    "start_training_time = time.time()\n",
    "\n",
    "for episode in range(10):\n",
    "    state = environment.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    #Checking metrics while training\n",
    "    state_freeze = state.copy()\n",
    "    num_updates = 0\n",
    "    num_moves = 0\n",
    "    invalid_moves = 0\n",
    "    start_episode_time = time.time()\n",
    "    \n",
    "    while not terminal:\n",
    "        #Core\n",
    "        action = agent.act(states=state)\n",
    "        state, terminal, reward = environment.execute(actions=action)\n",
    "        log2_reward = reward if reward <= 0 else np.log2(reward)\n",
    "        num_updates += agent.observe(terminal=terminal, reward=log2_reward)\n",
    "\n",
    "        #Number of moves\n",
    "        num_moves += 1\n",
    "        \n",
    "        #Number of invalid moves\n",
    "        if (state == state_freeze).all():\n",
    "            invalid_moves += 1\n",
    "        state_freeze = state.copy()\n",
    "    \n",
    "    # Storing score and max tile\n",
    "    max_tiles.append(environment._environment.environment.Matrix.max())\n",
    "    scores.append(environment._environment.environment.score)\n",
    "    \n",
    "    print('Episode {}: terminal = {}, updates={}, max_tile={}, valid_moves={}, invalid_moves={}, seconds={}'\\\n",
    "          .format(episode, terminal, num_updates, max_tiles[-1], num_moves-invalid_moves, invalid_moves, round(time.time() - start_episode_time,2)))\n",
    "\n",
    "agent.close()\n",
    "environment.close()\n",
    "print(\"Last 100 episodes mean score: \", np.mean(scores[-10:]))\n",
    "print(\"Max tile on last 100 episodes: \", max(max_tiles[-10:]))\n",
    "print(\"Total training time (minutes): \", round((time.time() - start_training_time)/60,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e946e72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:17:36.274448Z",
     "start_time": "2021-05-26T09:17:36.226611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 58033), started 0:09:05 ago. (Use '!kill 58033' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fec926e43d975a9e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fec926e43d975a9e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $logs_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595d177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T15:26:47.015574Z",
     "start_time": "2021-05-25T15:26:47.007913Z"
    }
   },
   "source": [
    "# Default hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(\n",
    "    environment='gym', level='2048-v0', max_episode_timesteps=1000\n",
    ")\n",
    "\n",
    "# Defaut hyperparams\n",
    "agent = Agent.create(\n",
    "    agent='dqn',\n",
    "    batch_size=16, # Required by Tensorforce\n",
    "    update_frequency=4, # Update frequency, TensorForce default : batch_size * 0.25\n",
    "    learning_rate = 0.001, # (TensorForce default)\n",
    "    discount = 0.99, # (TensorForce default)\n",
    "    memory=10000,\n",
    "    exploration=0.1, # (0 is the TensorForce default)\n",
    "    target_sync_frequency=4, # (1 is the TensorForce default)\n",
    ")\n",
    "\n",
    "for episode in range(1000):\n",
    "    train...\n",
    "    log2_reward = reward if reward <= 0 else np.log2(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
