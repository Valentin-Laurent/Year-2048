{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b184a774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T13:22:42.227452Z",
     "start_time": "2021-05-25T13:22:42.222872Z"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776219e",
   "metadata": {},
   "source": [
    "Collab specific stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d453b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T11:07:13.747963Z",
     "start_time": "2021-05-26T11:07:13.742118Z"
    }
   },
   "outputs": [],
   "source": [
    "running_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d954ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T11:07:14.658672Z",
     "start_time": "2021-05-26T11:07:14.653551Z"
    }
   },
   "outputs": [],
   "source": [
    "if running_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    logs_directory = 'drive/MyDrive/summaries'\n",
    "    !pip install -r \"drive/MyDrive/requirements.txt\"\n",
    "else:\n",
    "    logs_directory = 'summaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c5cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:44:26.531364Z",
     "start_time": "2021-05-26T07:44:25.843849Z"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c400da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T11:07:21.226646Z",
     "start_time": "2021-05-26T11:07:16.091460Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gym_2048\n",
    "from tensorforce import Agent, Environment\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99978760",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd57667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T11:07:21.241851Z",
     "start_time": "2021-05-26T11:07:21.230418Z"
    }
   },
   "outputs": [],
   "source": [
    "def mask_invalid_moves(state):\n",
    "    action_mask = []\n",
    "    for k in range(4):\n",
    "        try:\n",
    "            environment._environment.environment.move(k, trial=True)\n",
    "            action_mask.append(True)\n",
    "        except:\n",
    "            action_mask.append(False)\n",
    "    state_mask = dict(state=state,  action_mask=action_mask)\n",
    "    return state_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7ad4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T13:23:07.931146Z",
     "start_time": "2021-05-25T13:23:07.926389Z"
    }
   },
   "source": [
    "# Useful code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83046448",
   "metadata": {},
   "source": [
    "Render \"beautiful\" 2048 grid:\n",
    "\n",
    "`imshow(environment._environment.environment.render(mode=\"rgb_array\"))`\n",
    "\n",
    "Illegal move reward:\n",
    "\n",
    "`environment._environment.environment.set_illegal_move_reward(-10)`\n",
    "\n",
    "Log2 reward:\n",
    "\n",
    "`log2_reward = reward if reward <= 0 else np.log2(reward)`\n",
    "\n",
    "Show TensorBoard graphs:\n",
    "\n",
    "`%tensorboard --logdir summaries` (with parameter `summarizer=dict(directory='summaries')` in the `Agent.create()` method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40679b34",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Hyperparams to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7694a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`exploration`\n",
    "\n",
    "`learning_rate`\n",
    "\n",
    "Negative reward for illegal moves: `environment._environment.environment.set_illegal_move_reward(-1)`\n",
    "\n",
    "`target_sync_frequency`\n",
    "\n",
    "`batch_size` & `update_frequency`\n",
    "\n",
    "Constant reward / Log reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e12eea",
   "metadata": {},
   "source": [
    "# Custom hyper params train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125ff80",
   "metadata": {},
   "source": [
    "Modified hyperparams :\n",
    "\n",
    "Ex: `learning_rate = 0.01` et `batch_size = 32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0ac30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T11:07:53.208055Z",
     "start_time": "2021-05-26T11:07:51.336913Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e09dc80f1e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#Number of invalid moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstate_freeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0minvalid_moves\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mstate_freeze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "environment = Environment.create(\n",
    "    environment='gym', level='2048-v0', max_episode_timesteps=1000\n",
    ")\n",
    "\n",
    "# Defaut hyperparams\n",
    "agent = Agent.create(\n",
    "    agent='dqn',\n",
    "    batch_size=16, # Required by Tensorforce\n",
    "    update_frequency=4, # Update frequency, TensorForce default : batch_size * 0.25\n",
    "    environment=environment,\n",
    "    learning_rate = 0.001, # (TensorForce default)\n",
    "    discount = 0.99, # (TensorForce default)\n",
    "    memory=10000,\n",
    "    exploration=0.1, # (0 is the TensorForce default)\n",
    "    target_sync_frequency=4, # (1 is the TensorForce default)\n",
    "    #summarizer=dict(\n",
    "     #   directory=logs_directory,\n",
    "      #  summaries=[\n",
    "       #     \"regularization-loss\", \n",
    "        #    \"loss\",\n",
    "         #   \"episode-length\",\n",
    "          #  \"episode-reward\",\n",
    "           # \"objective-loss\",\n",
    "            #\"reward\",\n",
    "            #\"update-return\",\n",
    "        #]\n",
    "    #)\n",
    ")\n",
    "\n",
    "max_tiles = []\n",
    "scores = []\n",
    "start_training_time = time.time()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = environment.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    #Checking metrics while training\n",
    "    state_freeze = state.copy()\n",
    "    num_updates = 0\n",
    "    num_moves = 0\n",
    "    invalid_moves = 0\n",
    "    start_episode_time = time.time()\n",
    "    \n",
    "    while not terminal:\n",
    "        #Core\n",
    "        action = agent.act(states=mask_invalid_moves(state))\n",
    "        state, terminal, reward = environment.execute(actions=action)\n",
    "        log2_reward = reward if reward <= 0 else np.log2(reward)\n",
    "        num_updates += agent.observe(terminal=terminal, reward=log2_reward)\n",
    "\n",
    "        #Number of moves\n",
    "        num_moves += 1\n",
    "        \n",
    "        #Number of invalid moves\n",
    "        if (state == state_freeze).all():\n",
    "            invalid_moves += 1\n",
    "        state_freeze = state.copy()\n",
    "    \n",
    "    # Storing score and max tile\n",
    "    max_tiles.append(environment._environment.environment.Matrix.max())\n",
    "    scores.append(environment._environment.environment.score)\n",
    "    \n",
    "    print('Episode {}: terminal = {}, updates={}, max_tile={}, valid_moves={}, invalid_moves={}, seconds={}'\\\n",
    "          .format(episode, terminal, num_updates, max_tiles[-1], num_moves-invalid_moves, invalid_moves, round(time.time() - start_episode_time,2)))\n",
    "\n",
    "agent.close()\n",
    "environment.close()\n",
    "print(\"Last 100 episodes mean score: \", np.mean(scores[-100:]))\n",
    "print(\"Max tile on last 100 episodes: \", max(max_tiles[-100:]))\n",
    "print(\"Total training time (minutes): \", round((time.time() - start_training_time)/60,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc7b38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:17:36.274448Z",
     "start_time": "2021-05-26T09:17:36.226611Z"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir $logs_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a497f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T15:26:47.015574Z",
     "start_time": "2021-05-25T15:26:47.007913Z"
    }
   },
   "source": [
    "# Default hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ce4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(\n",
    "    environment='gym', level='2048-v0', max_episode_timesteps=1000\n",
    ")\n",
    "\n",
    "# Defaut hyperparams\n",
    "agent = Agent.create(\n",
    "    agent='dqn',\n",
    "    batch_size=16, # Required by Tensorforce\n",
    "    update_frequency=4, # Update frequency, TensorForce default : batch_size * 0.25\n",
    "    learning_rate = 0.001, # (TensorForce default)\n",
    "    discount = 0.99, # (TensorForce default)\n",
    "    memory=10000,\n",
    "    exploration=0.1, # (0 is the TensorForce default)\n",
    "    target_sync_frequency=4, # (1 is the TensorForce default)\n",
    ")\n",
    "\n",
    "for episode in range(1000):\n",
    "    train...\n",
    "    log2_reward = reward if reward <= 0 else np.log2(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
